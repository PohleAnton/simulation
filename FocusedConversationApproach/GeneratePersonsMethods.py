import shutil
import datetime
import chromadb
import openai
import yaml
import os
import json

with open('config.yml', 'r') as ymlfile:
    cfg = yaml.safe_load(ymlfile)
openai.api_key = cfg.get('openai')
os.environ['OPENAI_API_KEY'] = cfg.get('openai')

"""Diese Methode generiert die Profile und eine Konversation in einem Rutsch:"""


def generate_participants_and_conversation_from_scheme(participants, prompt, scheme_path):
    """
    Generates participant profiles and a conversation based on a provided scheme and a list of participants.

    :param participants: A list of strings, each representing a participant.
    :param prompt: A string to be used as the initial prompt for the conversation.
    :param scheme_path: A string representing the file path to the scheme.
    :return: A tuple containing two elements:
             1. The concatenated content of the generated profiles for each participant.
             2. The content of the final conversation as generated by OpenAI's ChatCompletion.
    """

    # Read the scheme from the file
    with open(scheme_path, 'r') as file:
        scheme = file.read()

    path = './FocusedConversationApproach/txtFiles/generatedProfiles/'
    if not os.path.exists(path):
        os.makedirs(path)

    target = './FocusedConversationApproach/txtFiles/generatedProfiles/used/'
    if not os.path.exists(target):
        os.makedirs(target)

    timestamp = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
    file_name = f"{timestamp}_{','.join(participants)}.txt"
    full_file_path = path + file_name

    with open(full_file_path, 'w') as file:
        file.write("")

    for participant in participants:
        intro = openai.ChatCompletion.create(
            model="gpt-3.5-turbo-1106",
            messages=[
                {"role": "user", "content": scheme + 'for' + participant}
            ]
        )
        with open(full_file_path, 'a') as file:
            file.write('\n\n')
            file.write(intro.choices[0].message.content)

    with open(full_file_path, 'r') as file:
        content = file.read()
        print("File Content:")

    for filename in os.listdir(path):
        if filename.endswith('.txt'):
            shutil.move(os.path.join(path, filename), os.path.join(target, filename))

    conversation = openai.ChatCompletion.create(
        model="gpt-3.5-turbo-1106",
        messages=[
            {"role": "user", "content": prompt + content}
        ]
    )

    return content, conversation['choices'][0]['message']['content']


"""example use"""

participants = ['Karl Marx', 'Peter Thiel', 'Elon Musk']
# wo auch immer das schema liegt...
scheme_path = './FocusedConversationApproach/txtFiles/scheme.txt'
prompt = (
    "Write a conversation with the following setup: "
    "1. Topics: At least two subjects in their interest. If the simulation hypothesis comes up, focus on that"
    "2. Informal, emotional conversation between people who’ve known each other for a long time and don’t like each other "
    "very much. They enjoy intense intellectual arguments and do not hold back.Deep Talk "
    "3. Long and detailed conversation. "
    "4. Setting: New Year‘s Eve Party. Both might have had a few drinks already "
    "5. Involved Individuals: "
)

both = generate_participants_and_conversation_from_scheme(participants, prompt, scheme_path)
print(both[0])
print(both[1])

"""diese Methode generiert nur Profile"""


def generate_participant_profiles(participants, scheme_path):
    """
    Generates participant profiles based on a provided scheme and a list of participants.

    :param participants: A list of strings, each representing a participant.
    :param scheme_path: A string representing the file path to the scheme.
    :return: The concatenated content of the generated profiles for each participant.
    """

    # Read the scheme from the file
    with open(scheme_path, 'r') as file:
        scheme = file.read()

    # Path for generated profiles
    path = './FocusedConversationApproach/txtFiles/generatedProfiles/'
    if not os.path.exists(path):
        os.makedirs(path)

    target = './FocusedConversationApproach/txtFiles/generatedProfiles/used/'
    if not os.path.exists(target):
        os.makedirs(target)

    # Create a unique file name based on timestamp and participants
    timestamp = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
    file_name = f"{timestamp}_{','.join(participants)}.txt"
    full_file_path = path + file_name

    # Write initial empty file
    with open(full_file_path, 'w') as file:
        file.write("")

    # Generate and append profiles for each participant
    for participant in participants:
        intro = openai.ChatCompletion.create(
            model="gpt-3.5-turbo-1106",
            messages=[
                {"role": "user", "content": scheme + 'for' + participant}
            ]
        )
        with open(full_file_path, 'a') as file:
            file.write('\n\n')
            file.write(intro.choices[0].message.content)

    # Read and return the file content
    with open(full_file_path, 'r') as file:
        content = file.read()

    for filename in os.listdir(path):
        if filename.endswith('.txt'):
            shutil.move(os.path.join(path, filename), os.path.join(target, filename))

    return content


"""example use:"""
participants = ['Karl Marx', 'Peter Thiel', 'Elon Musk']
# wo auch immer das schema liegt...
scheme_path = './FocusedConversationApproach/txtFiles/scheme.txt'
profiles = generate_participant_profiles(participants, scheme_path)

"""und super generisch, zum übergeben von 2 strings, also bspw. prompt + erinnerung aus der chroma:"""


def generate_conversation_from_content(prompt, content):
    """
    Generates a conversation based on the given prompt and pre-generated content.

    :param prompt: A string to be used as the initial prompt for the conversation.
    :param content: A string containing the pre-generated content (profiles).
    :return: The content of the final conversation as generated by OpenAI's ChatCompletion.
    """

    # Generate the final conversation
    conversation = openai.ChatCompletion.create(
        model="gpt-3.5-turbo-1106",
        messages=[
            {"role": "user", "content": prompt + content}
        ]
    )

    return conversation['choices'][0]['message']['content']


"""hier die function calls"""
functions = [
    {
        "name": "structure_conversation",
        "description": "A function divides a conversation by its themes, finds subtitles for each theme, and summarizes what each participant had to say about that subject",
        "parameters": {
            "type": "object",
            "properties": {
                "title": {
                    "type": "string",
                    "description": "Brief description of the conversation and who took part"
                },
                "themes": {
                    "type": "array",
                    "description": "A list of all the themes that came up",
                    "items": {
                        "type": "object",
                        "properties": {
                            "theme": {
                                "type": "string",
                                "description": "Each theme that has been brought up - what the most fitting Wikipedia-article might be called"
                            },
                            "content": {
                                "type": "array",
                                "description": "What each participant said about that subject",
                                "items": {
                                    "type": "object",
                                    "properties": {
                                        "name": {
                                            "type": "string",
                                            "description": "Name of the participant. Write like this: {name} thinks:"
                                        },
                                        "summary": {
                                            "type": "string",
                                            "description": "Summary of what they said about that topic. Start like this: \"I think...\""
                                        },
                                        "liking": {
                                            "type": "string",
                                            "description": "How much the participant liked that part of the conversation on a scale from 1 - 5, 1 being the lowest, 5 the highest score. Always write like this:  {name} gives it a {rating}"
                                        }
                                    }
                                }
                            }
                        }
                    }
                }
            }
        }
    },
]

"""diese Methode """


def analyze_conversation_with_functions(function_name, functions, conversation):
    """
    Processes a conversation string with a specified function and a set of additional functions
    using OpenAI's ChatCompletion API.

    :param function_name: A string representing the name of the primary function to be used in the analysis.
    :param functions: An array of additional functions to be used in the analysis.
    :param conversation: A string representing the conversation to be analyzed.
    :return: The JSON the function call produces.
    """

    # Call OpenAI's ChatCompletion API
    vector_test = openai.ChatCompletion.create(
        model="gpt-3.5-turbo-1106",
        messages=[
            {"role": "user", "content": function_name + conversation}
        ],
        functions=functions,
    )

    # Return the arguments from the function call in the API response
    return vector_test["choices"][0]["message"]["function_call"]["arguments"]


"""example use:"""
result = analyze_conversation_with_functions('structure_conversation', functions, both[1])
print(result)

"""diese methode returned eine Liste von Strings - diese sind Wikipedia-Überschriften wenigstens ähnlich. """


def wikiresult_from_function_call(result):
    """
    Processes a JSON string containing themed data and saves each theme's content to a file.
    Each file is named using a base title and the theme's title. Returns a list of Wikipedia-style
    bullet points derived from the themes.

    :param result: A JSON-formatted string containing the data to be processed.
    :return: A list of strings, each a Wikipedia-style bullet point derived from the themes.
    """

    # Parse the JSON data
    data = json.loads(result)

    # Create a base title for file naming
    base_title = data["title"].replace(' ', '_')

    # Create directories for storing files
    directory = './FocusedConversationApproach/txtFiles/ConversationChunks'
    if not os.path.exists(directory):
        os.makedirs(directory)
    target_dir = './FocusedConversationApproach/txtFiles/ConversationChunks/used/'
    if not os.path.exists(target_dir):
        os.makedirs(target_dir)

    wikipedia = []

    # Process each theme and save to files
    for theme in data["themes"]:
        theme_title = theme["theme"].replace(' ', '_').replace('\'', '').replace('\"', '').replace('?', '')
        filename = f"{base_title}_{theme_title}.txt"
        file_path = os.path.join(directory, filename)
        wikipedia.append(theme['theme'])

        if 'liking' in theme['content'][0]:
            content = '\n\n'.join(
                [f'{entry["name"]}:\n{entry["summary"]} {entry["liking"]}' for entry in theme["content"]])
        else:
            content = '\n\n'.join([f'{entry["name"]}:\n{entry["summary"]}' for entry in theme["content"]])

        with open(file_path, 'w') as file:
            file.write(content)

    return wikipedia

"""example use"""
wiki = wikiresult_from_function_call(result)
print(wiki[0])


###i need collection name

def chroma_collection_from_txt(collection_name, folder_path):
    chroma_path = './NetworkApproach/chromadb'
    os.makedirs(chroma_path, exist_ok=True)
    chroma=chromadb.PersistentClient(path=chroma_path)

    for collection in chroma.list_collections():
        if collection.name = collection_name:
            collection_name.add
            ##add to this collection
        else:
            ##create this collection
            ##then add to this collection.





    """
    Checks if a given name exists as the 'name' attribute of any collection in the provided list.

    :param collections: A list of Collection objects, each having a 'name' attribute.
    :param name_to_check: The string name to check for in the collections.
    :return: True if the name exists in the collections, False otherwise.
    """
    for collection in collections:
        if collection.name == name_to_check:
            return True
    return False

"""nun zu den datenbanken:"""

name_exists = collection_name_exists(chroma.list_collections(), "collection_name")
chroma =
chroma_path = './Conversations/chromadb'

if 'chroma' not in globals():
    chroma=chromadb.PersistentClient(path=chroma_path)
print(chroma.list_collections())
print('collection_nam' not in chroma.list_collections()[0])
gpt_split_db.list_collections()
if 'collection_for_chroma' not in globals():
    collection_for_chroma=chroma.create_collection('collection_name')


 def read_files_from_folder(folder_path):
     files = []
     for file_name in os.listdir(folder_path):
         if file_name.endswith(".txt"):
             with open(os.path.join(folder_path, file_name), 'r') as file:
                 contents = file.read()
                 files.append({"file_name": file_name, "content": contents})

     return files


 file_data = read_files_from_folder(directory)

 documents = []
 metadata = []
 ids = []


 for index, data in enumerate(file_data):
         documents.append(data['content'])
         metadata.append({'source': data['file_name']})
         ids.append(str(index + 1))

 # falls datenbank exisiert: bereits embedded dokumente zählen und auf index rechnen

 if 'gpt_split_db' in globals():
     for index, data in enumerate(file_data):
         documents.append(data['content'])
         metadata.append({'source': data['file_name']})
         ids.append(str(index + 1 + gpt_split_db.list_collections()[1].count()))

